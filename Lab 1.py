# -*- coding: utf-8 -*-
"""Untitled1_final_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C_EslJI1SoK1Z8yg_SGEkrx1MaIOenRl
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Checking the datsets**"""

import pandas as pd
train_dataframe=pd.read_csv('/content/drive/My Drive/train.csv')
valid_dataframe=pd.read_csv('/content/drive/My Drive/valid.csv')
test_dataframe=pd.read_csv('/content/drive/My Drive/X_test.csv')

print(train_dataframe.shape)
print(valid_dataframe.shape)
print(test_dataframe.shape)

import pandas as pd
dataframe=pd.read_csv('/content/drive/My Drive/train.csv')

"""## Missing values (30%)"""

missing_percentage = dataframe.isna().mean() * 100

# Filter columns with 30% or more missing values
missing_30 = missing_percentage[missing_percentage >=30].index.tolist()
print("Columns with 30% or more missing values:" )
# Print columns with 30% or more missing values
for column in missing_30:
  print(column)

dataframe=dataframe.drop(columns=missing_30)
print(dataframe.shape)

""" **Categorical/Numerical** **columns**"""

# Identify numeric and categorical columns
numeric_columns = dataframe.select_dtypes(include=['number']).columns.tolist()
categorical_columns = dataframe.select_dtypes(include=['object', 'category']).columns.tolist()

for x in numeric_columns:
  print(x)


for y in categorical_columns:
  print(y)

"""**Filling missing values**"""

# Fill missing values for numeric columns with mean or median
for col in numeric_columns:
    dataframe[col].fillna(dataframe[col].mean(), inplace=True)  # Use .median() for median

# Fill missing values for categorical columns with mode
for col in categorical_columns:
    dataframe[col].fillna(dataframe[col].mode()[0], inplace=True)



"""**drop categorical values**"""



"""**Scaling**

**Correlation**
"""

# Calculate correlation matrix
correlation_matrix =dataframe.corr()

# Print correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

correlated_columns = set()

for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.6:  # Adjust the threshold as needed
            col1 = correlation_matrix.columns[i]
            col2 = correlation_matrix.columns[j]
            if col1 not in correlated_columns:
                correlated_columns.add(col1)



print(correlated_columns)
print(len(correlated_columns))

#remove correlated columns

dataframe_filtered = dataframe.drop(columns=correlated_columns)

print(dataframe_filtered.shape)

"""**Outliers**"""

import matplotlib.pyplot as plt

dataframe_filtered.boxplot(figsize=(10, 6))  # Adjust figsize as needed
plt.title('Boxplots of Features')
plt.ylabel('Values')
plt.xlabel('Features')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.grid(True)  # Add gridlines for better visualization
plt.show()

"""Remove features"""

dataframe=pd.read_csv('/content/drive/My Drive/train.csv')

selected_features = [
    'loan_amnt', 'term', 'int_rate', 'annual_inc', 'verification_status',
    'dti', 'delinq_2yrs', 'pub_rec', 'revol_bal',
    'initial_list_status', 'total_pymnt', 'mort_acc', 'total_rec_prncp',
    'bc_util', 'total_il_high_credit_limit', 'loan_status'
]


new_dataframe =dataframe[selected_features]  #This new dataframe contains only the selected features

new_dataframe.head()

"""# **Feature Encoding**"""

# Mapping dictionary
mapping = {" 36 months": 36, " 60 months": 60}

new_dataframe["term"] = new_dataframe["term"].map(mapping)

# Renaming the column to "term_binary"
new_dataframe.rename(columns={"term": "term"}, inplace=True)

(new_dataframe["term"])

new_dataframe

# binary encoding for *initial_list_status_binary* column
# Mapping dictionary
mapping = {'f':1, 'w': 0}

# Applying binary encoding
new_dataframe['initial_list_status'] = new_dataframe['initial_list_status'].map(mapping)

new_dataframe['initial_list_status']

one_hot_encoded = pd.get_dummies(new_dataframe['verification_status'])

# Concatenate the one-hot encoded columns with the original DataFrame
new_dataframe = pd.concat([new_dataframe.drop('verification_status', axis=1), one_hot_encoded], axis=1)

# Move 'loan_status' column to the first position
#column_order = ['loan_status'] + [col for col in df_final.columns if col != 'loan_status']
#df_final = df_final[column_order]

new_dataframe

new_dataframe[['initial_list_status','term']]

from scipy.stats import zscore
import numpy as np



# Calculate the first quartile (Q1) and third quartile (Q3) for each feature
Q1 = new_dataframe.quantile(0.25)
Q3 = new_dataframe.quantile(0.75)

# Calculate the interquartile range (IQR) for each feature
IQR = Q3 - Q1

# Calculate the lower and upper bounds for outliers detection
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers for each feature
outliers = (new_dataframe < lower_bound) | (new_dataframe > upper_bound)

# Print the number of outliers for each feature
print("Number of outliers for each feature:")
print(outliers.sum())

"""## **Import Validation Data**"""

import pandas as pd
dataframe_valid = pd.read_csv('/content/drive/My Drive/valid.csv')

dataframe_valid.head()

#Take only the selected features from the validation dataset
final_dataframe_valid = dataframe_valid[['loan_amnt', 'term', 'int_rate', 'annual_inc', 'verification_status',    'dti', 'delinq_2yrs', 'pub_rec', 'revol_bal',    'initial_list_status', 'total_pymnt', 'mort_acc', 'total_rec_prncp',    'bc_util', 'total_il_high_credit_limit', 'loan_status']]



# Mapping dictionary
mapping = {" 36 months": 36, " 60 months": 60}

final_dataframe_valid["term"] = final_dataframe_valid["term"].map(mapping)

# Renaming the column to "term_binary"
final_dataframe_valid.rename(columns={"term": "term"}, inplace=True)

(final_dataframe_valid["term"])

# binary encoding initial_list_status_binary column
# Mapping dictionary
mapping = {'f': 1, 'w': 0}

# Applying binary encoding
final_dataframe_valid['initial_list_status'] = final_dataframe_valid['initial_list_status'].map(mapping)

final_dataframe_valid['initial_list_status']

one_hot_encoded = pd.get_dummies(final_dataframe_valid['verification_status'])

# Concatenate the one-hot encoded columns with the original DataFrame
final_dataframe_valid = pd.concat([final_dataframe_valid.drop('verification_status', axis=1), one_hot_encoded], axis=1)

final_dataframe_valid

"""# **XG boost**"""

import xgboost as xgb
from sklearn.metrics import accuracy_score

X_train = df_final.drop('loan_status', axis=1)
y_train = df_final['loan_status']

X_valid = final_dataframe_valid.drop('loan_status', axis=1)
y_valid =final_dataframe_valid['loan_status']

# Convert datasets into DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)

# Parameters for XGBoost
params = {
    'objective': 'binary:logistic',  # binary classification
    'eval_metric': 'logloss'  # Logarithmic Loss as evaluation metric
}

# Train the XGBoost model
num_rounds = 100  # Number of boosting rounds
model = xgb.train(params, dtrain, num_rounds, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions on validation data
y_pred = model.predict(dvalid)

# Convert probabilities to binary predictions
y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]

# Evaluate the model
accuracy = accuracy_score(y_valid, y_pred_binary)
print("Validation Accuracy:", accuracy)

"""## **Importing Test Data**"""

import pandas as pd
dataframe_test = pd.read_csv('/content/drive/My Drive/X_test.csv')
dataframe_test.head()

final_df_test = dataframe_test[ [ 'loan_amnt', 'term', 'int_rate', 'annual_inc', 'verification_status',    'dti', 'delinq_2yrs', 'pub_rec', 'revol_bal',    'initial_list_status', 'total_pymnt', 'mort_acc', 'total_rec_prncp',    'bc_util', 'total_il_high_credit_limit']]
final_df_test

# Mapping dictionary
mapping = {" 36 months": 36, " 60 months": 60}

final_df_test["term"] = final_df_test["term"].map(mapping)
final_df_test.rename(columns={"term": "term"}, inplace=True)

# binary encoding initial_list_status_binary column
# Mapping dictionary
mapping = {'f': 1, 'w': 0}

# Applying binary encoding
final_df_test['initial_list_status'] = final_df_test['initial_list_status'].map(mapping)

final_df_test['initial_list_status']

one_hot_encoded = pd.get_dummies(final_df_test['verification_status'])

# Concatenate the one-hot encoded columns with the original DataFrame
final_df_test = pd.concat([final_df_test.drop('verification_status', axis=1), one_hot_encoded], axis=1)

final_df_test

# Convert test dataset into DMatrix format
dtest = xgb.DMatrix(final_df_test)

# Make predictions on test data
y_pred_test = model.predict(dtest)

# Convert probabilities to binary predictions
y_pred_test_binary = [1 if pred > 0.5 else 0 for pred in y_pred_test]

import pandas as pd

# Convert the binary predictions to a pandas DataFrame
predictions_df = pd.DataFrame(y_pred_test_binary, columns=['predicted_loan_status'])

# Concatenate the predictions DataFrame to the front of final_df_test
final_df_test_with_predictions = pd.concat([predictions_df, final_df_test], axis=1)

final_df_test_with_predictions

csv_file_path = '/content/drive/My Drive/Output_210735U.csv'

# Save the dataframe as a CSV file
final_df_test_with_predictions.to_csv(csv_file_path, index=False)

pip install shap

import shap

# Explain the model's predictions using SHAP
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_train)

# Plot the SHAP summary plot
shap.summary_plot(shap_values, X_train, plot_type="bar", show=False)